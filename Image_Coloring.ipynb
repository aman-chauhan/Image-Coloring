{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image-Coloring.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/aman-chauhan/Image-Coloring/blob/miniCNN/Image_Coloring.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "AxyP8AK07EZv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Image-Coloring\n",
        "\n",
        "***Reference*** - [Let there be Color!: Joint End-to-end Learning of Global and Local Image Priors for Automatic Image Colorization with Simultaneous Classification - Satoshi Iizuka, Edgar Simo-Serra, Hiroshi Ishikawa](http://hi.cs.waseda.ac.jp/~iizuka/projects/colorization/en/)"
      ]
    },
    {
      "metadata": {
        "id": "Qwtukiv_k_bq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qlJ2JPlFiVni",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Authenticate"
      ]
    },
    {
      "metadata": {
        "id": "d5pG6hlliZ3o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PTBkHnNnidUH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "drive_service = build('drive', 'v3')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "URcxwckYY2vK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download data from Google Drive"
      ]
    },
    {
      "metadata": {
        "id": "gXjfNKx_Y7kh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e10771f9-6e05-4f49-b8ad-6c5ec612866f"
      },
      "cell_type": "code",
      "source": [
        "file_id = '11r-dOQ5Ve2dubFps6D-HTTi7urasqQqn'\n",
        "\n",
        "import io\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "\n",
        "request = drive_service.files().get_media(fileId=file_id)\n",
        "downloaded = io.BytesIO()\n",
        "downloader = MediaIoBaseDownload(downloaded, request)\n",
        "done = False\n",
        "while done is False:\n",
        "    progress, done = downloader.next_chunk()\n",
        "    print('.',end='')\n",
        "\n",
        "downloaded.seek(0)\n",
        "with open('data.zip','wb') as f:\n",
        "    f.write(downloaded.read())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "......................"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "StIgX6HWqDET",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!unzip -q data.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lyDp-PfuvCC-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm -r __MACOSX"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EI2bpZsUe5js",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Upload weights and epochs"
      ]
    },
    {
      "metadata": {
        "id": "BCgi9wNVgpj8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Upload? (1/0)\n",
        "option = 0 #@param\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "prUZsqmrgcG0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "if option==1:\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    for fn in uploaded.keys():\n",
        "        print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vBDR1-WJ7xR1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Library Verification"
      ]
    },
    {
      "metadata": {
        "id": "J3Ir9hbcwrXj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "dda8a4ae-6f9e-4713-9663-53b74f14a0fa"
      },
      "cell_type": "code",
      "source": [
        "!apt-get -qq install -y graphviz && pip install -q pydot\n",
        "!pip install imageio"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selecting previously unselected package fontconfig.\n",
            "(Reading database ... 18298 files and directories currently installed.)\n",
            "Preparing to unpack .../00-fontconfig_2.11.94-0ubuntu2_amd64.deb ...\n",
            "Unpacking fontconfig (2.11.94-0ubuntu2) ...\n",
            "Selecting previously unselected package libjbig0:amd64.\n",
            "Preparing to unpack .../01-libjbig0_2.1-3.1_amd64.deb ...\n",
            "Unpacking libjbig0:amd64 (2.1-3.1) ...\n",
            "Selecting previously unselected package libcdt5.\n",
            "Preparing to unpack .../02-libcdt5_2.38.0-16ubuntu2_amd64.deb ...\n",
            "Unpacking libcdt5 (2.38.0-16ubuntu2) ...\n",
            "Selecting previously unselected package libcgraph6.\n",
            "Preparing to unpack .../03-libcgraph6_2.38.0-16ubuntu2_amd64.deb ...\n",
            "Unpacking libcgraph6 (2.38.0-16ubuntu2) ...\n",
            "Selecting previously unselected package libtiff5:amd64.\n",
            "Preparing to unpack .../04-libtiff5_4.0.8-5ubuntu0.1_amd64.deb ...\n",
            "Unpacking libtiff5:amd64 (4.0.8-5ubuntu0.1) ...\n",
            "Selecting previously unselected package libwebp6:amd64.\n",
            "Preparing to unpack .../05-libwebp6_0.6.0-3_amd64.deb ...\n",
            "Unpacking libwebp6:amd64 (0.6.0-3) ...\n",
            "Selecting previously unselected package libxpm4:amd64.\n",
            "Preparing to unpack .../06-libxpm4_1%3a3.5.12-1_amd64.deb ...\n",
            "Unpacking libxpm4:amd64 (1:3.5.12-1) ...\n",
            "Selecting previously unselected package libgd3:amd64.\n",
            "Preparing to unpack .../07-libgd3_2.2.5-3_amd64.deb ...\n",
            "Unpacking libgd3:amd64 (2.2.5-3) ...\n",
            "Selecting previously unselected package libpixman-1-0:amd64.\n",
            "Preparing to unpack .../08-libpixman-1-0_0.34.0-1_amd64.deb ...\n",
            "Unpacking libpixman-1-0:amd64 (0.34.0-1) ...\n",
            "Selecting previously unselected package libxcb-render0:amd64.\n",
            "Preparing to unpack .../09-libxcb-render0_1.12-1ubuntu1_amd64.deb ...\n",
            "Unpacking libxcb-render0:amd64 (1.12-1ubuntu1) ...\n",
            "Selecting previously unselected package libxcb-shm0:amd64.\n",
            "Preparing to unpack .../10-libxcb-shm0_1.12-1ubuntu1_amd64.deb ...\n",
            "Unpacking libxcb-shm0:amd64 (1.12-1ubuntu1) ...\n",
            "Selecting previously unselected package libcairo2:amd64.\n",
            "Preparing to unpack .../11-libcairo2_1.14.10-1ubuntu1_amd64.deb ...\n",
            "Unpacking libcairo2:amd64 (1.14.10-1ubuntu1) ...\n",
            "Selecting previously unselected package libltdl7:amd64.\n",
            "Preparing to unpack .../12-libltdl7_2.4.6-2_amd64.deb ...\n",
            "Unpacking libltdl7:amd64 (2.4.6-2) ...\n",
            "Selecting previously unselected package libthai-data.\n",
            "Preparing to unpack .../13-libthai-data_0.1.26-3_all.deb ...\n",
            "Unpacking libthai-data (0.1.26-3) ...\n",
            "Selecting previously unselected package libdatrie1:amd64.\n",
            "Preparing to unpack .../14-libdatrie1_0.2.10-5_amd64.deb ...\n",
            "Unpacking libdatrie1:amd64 (0.2.10-5) ...\n",
            "Selecting previously unselected package libthai0:amd64.\n",
            "Preparing to unpack .../15-libthai0_0.1.26-3_amd64.deb ...\n",
            "Unpacking libthai0:amd64 (0.1.26-3) ...\n",
            "Selecting previously unselected package libpango-1.0-0:amd64.\n",
            "Preparing to unpack .../16-libpango-1.0-0_1.40.12-1_amd64.deb ...\n",
            "Unpacking libpango-1.0-0:amd64 (1.40.12-1) ...\n",
            "Selecting previously unselected package libgraphite2-3:amd64.\n",
            "Preparing to unpack .../17-libgraphite2-3_1.3.10-2_amd64.deb ...\n",
            "Unpacking libgraphite2-3:amd64 (1.3.10-2) ...\n",
            "Selecting previously unselected package libharfbuzz0b:amd64.\n",
            "Preparing to unpack .../18-libharfbuzz0b_1.4.2-1_amd64.deb ...\n",
            "Unpacking libharfbuzz0b:amd64 (1.4.2-1) ...\n",
            "Selecting previously unselected package libpangoft2-1.0-0:amd64.\n",
            "Preparing to unpack .../19-libpangoft2-1.0-0_1.40.12-1_amd64.deb ...\n",
            "Unpacking libpangoft2-1.0-0:amd64 (1.40.12-1) ...\n",
            "Selecting previously unselected package libpangocairo-1.0-0:amd64.\n",
            "Preparing to unpack .../20-libpangocairo-1.0-0_1.40.12-1_amd64.deb ...\n",
            "Unpacking libpangocairo-1.0-0:amd64 (1.40.12-1) ...\n",
            "Selecting previously unselected package libpathplan4.\n",
            "Preparing to unpack .../21-libpathplan4_2.38.0-16ubuntu2_amd64.deb ...\n",
            "Unpacking libpathplan4 (2.38.0-16ubuntu2) ...\n",
            "Selecting previously unselected package libgvc6.\n",
            "Preparing to unpack .../22-libgvc6_2.38.0-16ubuntu2_amd64.deb ...\n",
            "Unpacking libgvc6 (2.38.0-16ubuntu2) ...\n",
            "Selecting previously unselected package libgvpr2.\n",
            "Preparing to unpack .../23-libgvpr2_2.38.0-16ubuntu2_amd64.deb ...\n",
            "Unpacking libgvpr2 (2.38.0-16ubuntu2) ...\n",
            "Selecting previously unselected package libxt6:amd64.\n",
            "Preparing to unpack .../24-libxt6_1%3a1.1.5-1_amd64.deb ...\n",
            "Unpacking libxt6:amd64 (1:1.1.5-1) ...\n",
            "Selecting previously unselected package libxmu6:amd64.\n",
            "Preparing to unpack .../25-libxmu6_2%3a1.1.2-2_amd64.deb ...\n",
            "Unpacking libxmu6:amd64 (2:1.1.2-2) ...\n",
            "Selecting previously unselected package libxaw7:amd64.\n",
            "Preparing to unpack .../26-libxaw7_2%3a1.0.13-1_amd64.deb ...\n",
            "Unpacking libxaw7:amd64 (2:1.0.13-1) ...\n",
            "Selecting previously unselected package graphviz.\n",
            "Preparing to unpack .../27-graphviz_2.38.0-16ubuntu2_amd64.deb ...\n",
            "Unpacking graphviz (2.38.0-16ubuntu2) ...\n",
            "Setting up libpathplan4 (2.38.0-16ubuntu2) ...\n",
            "Setting up libxcb-render0:amd64 (1.12-1ubuntu1) ...\n",
            "Setting up libjbig0:amd64 (2.1-3.1) ...\n",
            "Setting up libdatrie1:amd64 (0.2.10-5) ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting up libtiff5:amd64 (4.0.8-5ubuntu0.1) ...\r\n",
            "Setting up libgraphite2-3:amd64 (1.3.10-2) ...\n",
            "Setting up libpixman-1-0:amd64 (0.34.0-1) ...\n",
            "Processing triggers for libc-bin (2.26-0ubuntu2.1) ...\n",
            "Setting up libltdl7:amd64 (2.4.6-2) ...\n",
            "Setting up libxcb-shm0:amd64 (1.12-1ubuntu1) ...\n",
            "Setting up libxpm4:amd64 (1:3.5.12-1) ...\n",
            "Setting up libxt6:amd64 (1:1.1.5-1) ...\n",
            "Setting up libthai-data (0.1.26-3) ...\n",
            "Setting up libcdt5 (2.38.0-16ubuntu2) ...\n",
            "Setting up fontconfig (2.11.94-0ubuntu2) ...\n",
            "Regenerating fonts cache... done.\n",
            "Setting up libcgraph6 (2.38.0-16ubuntu2) ...\n",
            "Setting up libwebp6:amd64 (0.6.0-3) ...\n",
            "Setting up libcairo2:amd64 (1.14.10-1ubuntu1) ...\n",
            "Setting up libgvpr2 (2.38.0-16ubuntu2) ...\n",
            "Setting up libgd3:amd64 (2.2.5-3) ...\n",
            "Setting up libharfbuzz0b:amd64 (1.4.2-1) ...\n",
            "Setting up libthai0:amd64 (0.1.26-3) ...\n",
            "Setting up libxmu6:amd64 (2:1.1.2-2) ...\n",
            "Setting up libpango-1.0-0:amd64 (1.40.12-1) ...\n",
            "Setting up libxaw7:amd64 (2:1.0.13-1) ...\n",
            "Setting up libpangoft2-1.0-0:amd64 (1.40.12-1) ...\n",
            "Setting up libpangocairo-1.0-0:amd64 (1.40.12-1) ...\n",
            "Setting up libgvc6 (2.38.0-16ubuntu2) ...\n",
            "Setting up graphviz (2.38.0-16ubuntu2) ...\n",
            "Processing triggers for libc-bin (2.26-0ubuntu2.1) ...\n",
            "Collecting imageio\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a7/1d/33c8686072148b3b0fcc12a2e0857dd8316b8ae20a0fa66c8d6a6d01c05c/imageio-2.3.0-py2.py3-none-any.whl (3.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.3MB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from imageio) (1.14.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio) (4.0.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->imageio) (0.45.1)\n",
            "Installing collected packages: imageio\n",
            "Successfully installed imageio-2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mO6emBpv8Ws1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "metadata": {
        "id": "MSyOVdx_8F0W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "141cb081-6ab8-4509-bb3d-91e19b6fffaf"
      },
      "cell_type": "code",
      "source": [
        "# Keras Libraries\n",
        "from keras.models import Model\n",
        "\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.convolutional import UpSampling2D\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Lambda\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers.merge import Add\n",
        "from keras.layers import Input\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils import plot_model\n",
        "from keras.utils import Sequence\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import Callback\n",
        "\n",
        "from keras import regularizers\n",
        "from keras import backend as K\n",
        "\n",
        "# Other libraries\n",
        "from skimage.color import rgb2lab\n",
        "from skimage.color import lab2rgb\n",
        "from imageio import imwrite\n",
        "from imageio import imread\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import json\n",
        "import os"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Z6cLDRLNByT1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## CNN Models"
      ]
    },
    {
      "metadata": {
        "id": "PCq5xaItCL2C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Low-Level Feature Network"
      ]
    },
    {
      "metadata": {
        "id": "Bv_cA5xDCK0M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def llfn():\n",
        "    # Input tensor\n",
        "    llfn_input = Input(batch_shape=(None, None, None, 1), name='llfn_input')\n",
        "\n",
        "    # Convolutional Layer with 32 3x3 kernels with double stride and same padding\n",
        "    llfn_conv1 = Conv2D(filters=32, kernel_size=3, strides=2, padding='same', activation='relu',\n",
        "                        kernel_initializer='he_uniform', bias_initializer='he_uniform', kernel_regularizer=regularizers.l2(0.0005))(llfn_input)\n",
        "    llfn_conv1 = BatchNormalization()(llfn_conv1)\n",
        "\n",
        "    # Convolutional Layer with 64 3x3 kernels with single stride and same padding\n",
        "    llfn_conv2 = Conv2D(filters=64, kernel_size=3, strides=1, padding='same', activation='relu',\n",
        "                        kernel_initializer='he_uniform', bias_initializer='he_uniform', kernel_regularizer=regularizers.l2(0.0005))(llfn_conv1)\n",
        "    llfn_conv2 = BatchNormalization()(llfn_conv2)\n",
        "\n",
        "    # Convolutional Layer with 64 3x3 kernels with double stride and same padding\n",
        "    llfn_conv3 = Conv2D(filters=64, kernel_size=3, strides=2, padding='same', activation='relu',\n",
        "                        kernel_initializer='he_uniform', bias_initializer='he_uniform', kernel_regularizer=regularizers.l2(0.0005))(llfn_conv2)\n",
        "    llfn_conv3 = BatchNormalization()(llfn_conv3)\n",
        "\n",
        "    # Convolutional Layer with 128 3x3 kernels with single stride and same padding\n",
        "    llfn_conv4 = Conv2D(filters=128, kernel_size=3, strides=1, padding='same', activation='relu',\n",
        "                        kernel_initializer='he_uniform', bias_initializer='he_uniform', kernel_regularizer=regularizers.l2(0.0005))(llfn_conv3)\n",
        "    llfn_conv4 = BatchNormalization()(llfn_conv4)\n",
        "\n",
        "    # Convolutional Layer with 128 3x3 kernels with double stride and same padding\n",
        "    llfn_conv5 = Conv2D(filters=128, kernel_size=3, strides=2, padding='same', activation='relu',\n",
        "                        kernel_initializer='he_uniform', bias_initializer='he_uniform', kernel_regularizer=regularizers.l2(0.0005))(llfn_conv4)\n",
        "    llfn_conv5 = BatchNormalization()(llfn_conv5)\n",
        "\n",
        "    # Convolutional Layer with 256 3x3 kernels with single stride and same padding\n",
        "    llfn_conv6 = Conv2D(filters=256, kernel_size=3, strides=1, padding='same', activation='relu',\n",
        "                        kernel_initializer='he_uniform', bias_initializer='he_uniform', kernel_regularizer=regularizers.l2(0.0005))(llfn_conv5)\n",
        "    llfn_conv6 = BatchNormalization()(llfn_conv6)\n",
        "\n",
        "    # Model definition\n",
        "    llfn_model = Model(inputs=llfn_input, outputs=llfn_conv6)\n",
        "\n",
        "    return llfn_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jef__jYXHN4P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Mid-Level Feature Network"
      ]
    },
    {
      "metadata": {
        "id": "g6g5O3g6RWsx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def mlfn():\n",
        "    # Input tensor\n",
        "    mlfn_input = Input(batch_shape=(None, None, None, 256), name='mlfn_input')\n",
        "\n",
        "    # Convolutional Layer with 256 3x3 kernels with single stride and same padding\n",
        "    mlfn_conv1 = Conv2D(filters=256, kernel_size=3, strides=1, padding='same', activation='relu',\n",
        "                        kernel_initializer='he_uniform', bias_initializer='he_uniform', kernel_regularizer=regularizers.l2(0.0005))(mlfn_input)\n",
        "    mlfn_conv1 = BatchNormalization()(mlfn_conv1)\n",
        "\n",
        "    # Convolutional Layer with 256 3x3 kernels with single stride and same padding\n",
        "    mlfn_conv2 = Conv2D(filters=256, kernel_size=3, strides=1, padding='same', activation='relu',\n",
        "                        kernel_initializer='he_uniform', bias_initializer='he_uniform', kernel_regularizer=regularizers.l2(0.0005))(mlfn_conv1)\n",
        "    mlfn_conv2 = BatchNormalization()(mlfn_conv2)\n",
        "\n",
        "    # Model definition\n",
        "    mlfn_model = Model(inputs=mlfn_input, outputs=mlfn_conv2, name='mlfn_model')\n",
        "\n",
        "    return mlfn_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KQFRK5d6RlZL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Global Feature Network"
      ]
    },
    {
      "metadata": {
        "id": "QsPxihUKReHf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gfn():\n",
        "    # Input tensor\n",
        "    gfn_input = Input(batch_shape=(None, 28, 28, 256), name='gfn_input')\n",
        "\n",
        "    # Convolutional Layer with 256 3x3 kernels with double stride and same padding\n",
        "    gfn_conv1 = Conv2D(filters=256, kernel_size=3, strides=2, padding='same', activation='relu',\n",
        "                       kernel_initializer='he_uniform', bias_initializer='he_uniform', kernel_regularizer=regularizers.l2(0.0005))(gfn_input)\n",
        "    gfn_conv1 = BatchNormalization()(gfn_conv1)\n",
        "\n",
        "    # Convolutional Layer with 256 3x3 kernels with single stride and same padding\n",
        "    gfn_conv2 = Conv2D(filters=256, kernel_size=3, strides=1, padding='same', activation='relu',\n",
        "                       kernel_initializer='he_uniform', bias_initializer='he_uniform', kernel_regularizer=regularizers.l2(0.0005))(gfn_conv1)\n",
        "    gfn_conv2 = BatchNormalization()(gfn_conv2)\n",
        "\n",
        "    # Convolutional Layer with 256 3x3 kernels with single stride and same padding\n",
        "    gfn_conv3 = Conv2D(filters=256, kernel_size=3, strides=2, padding='same', activation='relu',\n",
        "                       kernel_initializer='he_uniform', bias_initializer='he_uniform', kernel_regularizer=regularizers.l2(0.0005))(gfn_conv2)\n",
        "    gfn_conv3 = BatchNormalization()(gfn_conv3)\n",
        "\n",
        "    # Convolutional Layer with 256 3x3 kernels with single stride and same padding\n",
        "    gfn_conv4 = Conv2D(filters=256, kernel_size=3, strides=1, padding='same', activation='relu',\n",
        "                       kernel_initializer='he_uniform', bias_initializer='he_uniform', kernel_regularizer=regularizers.l2(0.0005))(gfn_conv3)\n",
        "    gfn_conv4 = BatchNormalization()(gfn_conv4)\n",
        "\n",
        "    # Flatten the layer\n",
        "    gfn_flttn = Flatten()(gfn_conv4)\n",
        "\n",
        "    # Fully Connected Layer with 1024 units\n",
        "    gfn_fcon1 = Dense(units=1024, activation='relu', kernel_initializer='he_uniform',\n",
        "                      bias_initializer='he_uniform', kernel_regularizer=regularizers.l2(0.0005))(gfn_flttn)\n",
        "    gfn_fcon1 = BatchNormalization()(gfn_fcon1)\n",
        "    gfn_fcon1 = Dropout(0.20)(gfn_fcon1)\n",
        "\n",
        "    # Fully Connected Layer with 512 units\n",
        "    gfn_fcon2 = Dense(units=512, activation='relu', kernel_initializer='he_uniform',\n",
        "                      bias_initializer='he_uniform', kernel_regularizer=regularizers.l2(0.0005))(gfn_fcon1)\n",
        "    gfn_fcon2 = BatchNormalization()(gfn_fcon2)\n",
        "    gfn_fcon2 = Dropout(0.20)(gfn_fcon2)\n",
        "\n",
        "    # Model definition\n",
        "    gfn_model = Model(inputs=gfn_input, outputs=gfn_fcon2, name='gfn_model')\n",
        "\n",
        "    return gfn_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XQ72nXfARtF8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Classifier Network"
      ]
    },
    {
      "metadata": {
        "id": "-hOvsWsqRveD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def clf():\n",
        "    # Input tensor\n",
        "    clf_input = Input(batch_shape=(None, 512), name='clf_input')\n",
        "\n",
        "    # Fully Connected Layer with 256 units\n",
        "    clf_fcon1 = Dense(units=256, activation='relu', kernel_initializer='he_uniform',\n",
        "                      bias_initializer='he_uniform', kernel_regularizer=regularizers.l2(0.0005))(clf_input)\n",
        "    clf_fcon1 = BatchNormalization()(clf_fcon1)\n",
        "    clf_fcon1 = Dropout(0.20)(clf_fcon1)\n",
        "\n",
        "    # Fully Connected Layer with 'output' units\n",
        "    clf_fcon2 = Dense(units=719, activation='softmax', kernel_initializer='he_uniform',\n",
        "                      bias_initializer='he_uniform', kernel_regularizer=regularizers.l2(0.0005))(clf_fcon1)\n",
        "\n",
        "    # Model definition\n",
        "    clf_model = Model(inputs=clf_input, outputs=clf_fcon2, name='clf_model')\n",
        "\n",
        "    return clf_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UjmSn0kdRyYx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Color Network"
      ]
    },
    {
      "metadata": {
        "id": "GaKqz1eDRzsk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def color():\n",
        "    # Input tensor\n",
        "    color_input = Input(batch_shape=(None, None, None, 256), name='color_input')\n",
        "\n",
        "    # Convolutional Layer with 128 3x3 kernels with single stride and same padding\n",
        "    color_conv1 = Conv2D(filters=128, kernel_size=3, strides=1, padding='same', activation='relu',\n",
        "                         kernel_initializer='he_uniform', bias_initializer='he_uniform', kernel_regularizer=regularizers.l2(0.0005))(color_input)\n",
        "    color_conv1 = BatchNormalization()(color_conv1)\n",
        "\n",
        "    # Convolutional Layer with 64 3x3 kernels with single stride and same padding\n",
        "    color_conv2 = Conv2D(filters=64, kernel_size=3, strides=1, padding='same', activation='relu',\n",
        "                         kernel_initializer='he_uniform', bias_initializer='he_uniform', kernel_regularizer=regularizers.l2(0.0005))(color_conv1)\n",
        "    color_conv2 = BatchNormalization()(color_conv2)\n",
        "\n",
        "    # Upsampling\n",
        "    color_upsm1 = UpSampling2D(size=2)(color_conv2)\n",
        "\n",
        "    # Convolutional Layer with 64 3x3 kernels with single stride and same padding\n",
        "    color_conv3 = Conv2D(filters=64, kernel_size=3, strides=1, padding='same', activation='relu',\n",
        "                         kernel_initializer='he_uniform', bias_initializer='he_uniform', kernel_regularizer=regularizers.l2(0.0005))(color_upsm1)\n",
        "    color_conv3 = BatchNormalization()(color_conv3)\n",
        "\n",
        "    # Convolutional Layer with 32 3x3 kernels with single stride and same padding\n",
        "    color_conv4 = Conv2D(filters=32, kernel_size=3, strides=1, padding='same', activation='relu',\n",
        "                         kernel_initializer='he_uniform', bias_initializer='he_uniform', kernel_regularizer=regularizers.l2(0.0005))(color_conv3)\n",
        "    color_conv4 = BatchNormalization()(color_conv4)\n",
        "\n",
        "    # Upsampling\n",
        "    color_upsm2 = UpSampling2D(size=2)(color_conv4)\n",
        "\n",
        "    # Convolutional Layer with 32 3x3 kernels with single stride and same padding\n",
        "    color_conv5 = Conv2D(filters=32, kernel_size=3, strides=1, padding='same', activation='relu',\n",
        "                         kernel_initializer='he_uniform', bias_initializer='he_uniform', kernel_regularizer=regularizers.l2(0.0005))(color_upsm2)\n",
        "    color_conv5 = BatchNormalization()(color_conv5)\n",
        "\n",
        "    # Convolutional Layer with 16 3x3 kernels with single stride and same padding\n",
        "    color_conv6 = Conv2D(filters=16, kernel_size=3, strides=1, padding='same', activation='relu',\n",
        "                         kernel_initializer='he_uniform', bias_initializer='he_uniform', kernel_regularizer=regularizers.l2(0.0005))(color_conv5)\n",
        "    color_conv6 = BatchNormalization()(color_conv6)\n",
        "\n",
        "    # Upsampling\n",
        "    color_upsm3 = UpSampling2D(size=2)(color_conv6)\n",
        "\n",
        "    # Convolutional Layer with 2 3x3 kernels with single stride and same padding\n",
        "    color_conv7 = Conv2D(filters=2, kernel_size=3, strides=1, padding='same',\n",
        "                         activation='sigmoid', kernel_initializer='he_uniform', bias_initializer='he_uniform', kernel_regularizer=regularizers.l2(0.0005))(color_upsm3)\n",
        "\n",
        "    # Model definition\n",
        "    color_model = Model(inputs=color_input, outputs=color_conv7, name='color_model')\n",
        "\n",
        "    return color_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2fl8IgDYTTzn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Full Network"
      ]
    },
    {
      "metadata": {
        "id": "0mh7ip7WTZHf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tile(x, k):\n",
        "    x = K.expand_dims(x, 1)\n",
        "    x = K.expand_dims(x, 1)\n",
        "    x = K.tile(x, [1, k[1], k[2], 1])\n",
        "    return x\n",
        "\n",
        "\n",
        "def model():\n",
        "    color_input = Input(batch_shape=(None, None, None, 1), name='global_color')\n",
        "    color_branch = llfn()(color_input)\n",
        "    color_branch = mlfn()(color_branch)\n",
        "\n",
        "    class_input = Input(batch_shape=(None, 224, 224, 1), name='global_class')\n",
        "    class_branch = llfn()(class_input)\n",
        "    class_branch = gfn()(class_branch)\n",
        "\n",
        "    gfn_units = Dense(units=256, activation='relu', kernel_initializer='he_uniform',\n",
        "                      bias_initializer='he_uniform', kernel_regularizer=regularizers.l2(0.0005))(class_branch)\n",
        "    gfn_units = BatchNormalization()(gfn_units)\n",
        "\n",
        "    color_branch = Add()([color_branch, Lambda(tile, arguments={'k': K.shape(color_branch)})(gfn_units)])\n",
        "    color_branch = color()(color_branch)\n",
        "\n",
        "    class_branch = clf()(class_branch)\n",
        "\n",
        "    model = Model(inputs=[color_input, class_input], outputs=[color_branch, class_branch], name='global_model')\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9v0zseHaR3U9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Generator Class"
      ]
    },
    {
      "metadata": {
        "id": "sAOyCrWbTJuq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DataGenerator(Sequence):\n",
        "    def __init__(self, list_IDs, partition, labels, batch_size=28, n_channel=1, n_classes=719, shuffle=True, augment=False):\n",
        "        if augment:\n",
        "            self.list_IDs = list_IDs * 2\n",
        "        else:\n",
        "            self.list_IDs = list_IDs\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "        self.n_channel = n_channel\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.augment = augment\n",
        "        self.partition = partition\n",
        "        self.prng = np.random.RandomState(42)\n",
        "        self.datagen = ImageDataGenerator(rotation_range=45, width_shift_range=0.15, height_shift_range=0.15, shear_range=0.15,\n",
        "                                          fill_mode='constant', cval=0, zoom_range=0.15, horizontal_flip=True)\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            self.prng.shuffle(self.indexes)\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
        "        return self.__data_generation(list_IDs_temp)\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        X = np.empty((self.batch_size, 224, 224, self.n_channel))\n",
        "        Y = np.empty((self.batch_size, 224, 224, 2))\n",
        "        y = np.empty((self.batch_size,), dtype=int)\n",
        "\n",
        "        for i, ID in enumerate(list_IDs_temp):\n",
        "            if not self.augment:\n",
        "                X[i] = np.expand_dims(\n",
        "                    (imread(os.path.join(os.path.join('data', self.partition), ID)) - 127.5) / 127.5, axis=-1)\n",
        "                Y[i] = (rgb2lab(imread(os.path.join(os.path.join('data', self.partition + '-target'), ID)))\n",
        "                        [:, :, 1:] + 128.0) / (255.0)\n",
        "                y[i] = self.labels[ID]\n",
        "            else:\n",
        "                seed = self.prng.randint(0, 1000000)\n",
        "                X[i] = (self.datagen.random_transform(np.expand_dims(\n",
        "                    imread(os.path.join(os.path.join('data', self.partition), ID)), axis=-1), seed=seed) - 127.5) / 127.5\n",
        "                Y[i] = (rgb2lab(self.datagen.random_transform(imread(os.path.join(os.path.join('data', self.partition + '-target'), ID)), seed=seed))\n",
        "                        [:, :, 1:] + 128.0) / (255.0)\n",
        "                y[i] = self.labels[ID]\n",
        "        return ([X, X], [Y, to_categorical(y, num_classes=self.n_classes)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Un3PccfGTpzt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Save Callback Class"
      ]
    },
    {
      "metadata": {
        "id": "UdUK3eCrTxya",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SaveCallback(Callback):\n",
        "    def __init__(self, model):\n",
        "        self.model_to_save = model\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        self.model_to_save.save_weights('weights.h5')\n",
        "        d = {}\n",
        "        if os.path.exists('epochs.json'):\n",
        "            d = json.load(open('epochs.json'))\n",
        "        d[epoch] = logs\n",
        "        json.dump(d, open('epochs.json', 'w'))\n",
        "        files.download('epochs.json')\n",
        "        files.download('weights.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N5aFAy7zT0nc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train Method"
      ]
    },
    {
      "metadata": {
        "id": "GGjqj2VTT4I3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    labels = json.load(open(os.path.join('data', 'labels.json')))\n",
        "    partition = {'training': None, 'validation': None}\n",
        "    for x in partition.keys():\n",
        "        partition[x] = [f for f in os.listdir(os.path.join('data', x)) if os.path.isfile(\n",
        "            os.path.join(os.path.join('data', x), f))]\n",
        "        partition[x].sort()\n",
        "    print('Indices read.')\n",
        "\n",
        "    n_classes = len({labels[x] for x in labels})\n",
        "    l = {labels[x] for x in labels}\n",
        "    l = {x: i for i, x in enumerate(sorted(list(l)))}\n",
        "    labels = {x: l[labels[x]] for x in labels.keys()}\n",
        "    json.dump(l, open('mapping.json', 'w'))\n",
        "    print('Mappings written.')\n",
        "\n",
        "    training_generator = DataGenerator(partition['training'], 'training', labels, 64, 1, n_classes, True, True)\n",
        "    validation_generator = DataGenerator(partition['validation'], 'validation', labels, 64, 1, n_classes, True, True)\n",
        "\n",
        "    fmodel = model()\n",
        "    if os.path.exists('weights.h5'):\n",
        "        fmodel.load_weights('weights.h5')\n",
        "\n",
        "    initial_epoch = 0\n",
        "    if os.path.exists('epochs.json'):\n",
        "        initial_epoch = len(json.load(open('epochs.json')).keys())\n",
        "\n",
        "    cbk = SaveCallback(fmodel)\n",
        "    fmodel.compile(optimizer='adadelta', loss={\n",
        "        'color_model': 'mean_squared_error', 'clf_model': 'categorical_crossentropy'}, metrics={'color_model': 'accuracy', 'clf_model': 'accuracy'})\n",
        "    fmodel.fit_generator(generator=training_generator, epochs=20, verbose=1, callbacks=[\n",
        "        cbk], validation_data=validation_generator, use_multiprocessing=True, workers=4, initial_epoch=initial_epoch)\n",
        "    print('Training done.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oG1JquQyvwM3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4451
        },
        "outputId": "51928641-b04b-460a-ffc7-afc121008263"
      },
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Indices read.\n",
            "Mappings written.\n",
            "Epoch 1/20\n",
            "467/561 [=======================>......] - ETA: 3:57 - loss: 10.3425 - color_model_loss: 0.0055 - clf_model_loss: 5.5082 - color_model_acc: 0.5414 - clf_model_acc: 0.1534"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "561/561 [==============================] - 1561s 3s/step - loss: 9.9908 - color_model_loss: 0.0050 - clf_model_loss: 5.4151 - color_model_acc: 0.5452 - clf_model_acc: 0.1570 - val_loss: 7.8333 - val_color_model_loss: 0.0024 - val_clf_model_loss: 4.7578 - val_color_model_acc: 0.5313 - val_clf_model_acc: 0.1926\n",
            "Epoch 2/20\n",
            "169/561 [========>.....................] - ETA: 15:42 - loss: 7.5434 - color_model_loss: 0.0023 - clf_model_loss: 4.7898 - color_model_acc: 0.5804 - clf_model_acc: 0.1930"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "561/561 [==============================] - 1487s 3s/step - loss: 6.8653 - color_model_loss: 0.0023 - clf_model_loss: 4.6728 - color_model_acc: 0.5983 - clf_model_acc: 0.1989 - val_loss: 5.9869 - val_color_model_loss: 0.0023 - val_clf_model_loss: 4.4458 - val_color_model_acc: 0.5423 - val_clf_model_acc: 0.2207\n",
            "Epoch 3/20\n",
            " 67/561 [==>...........................] - ETA: 17:08 - loss: 5.8845 - color_model_loss: 0.0021 - clf_model_loss: 4.3945 - color_model_acc: 0.6196 - clf_model_acc: 0.2222"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "561/561 [==============================] - 1430s 3s/step - loss: 5.6101 - color_model_loss: 0.0021 - clf_model_loss: 4.3991 - color_model_acc: 0.6134 - clf_model_acc: 0.2245 - val_loss: 5.4426 - val_color_model_loss: 0.0022 - val_clf_model_loss: 4.4719 - val_color_model_acc: 0.5585 - val_clf_model_acc: 0.2122\n",
            "Epoch 4/20\n",
            " 17/561 [..............................] - ETA: 20:24 - loss: 5.3481 - color_model_loss: 0.0022 - clf_model_loss: 4.3816 - color_model_acc: 0.6005 - clf_model_acc: 0.2316"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Process ForkPoolWorker-40:\n",
            "Process ForkPoolWorker-29:\n",
            "Process ForkPoolWorker-31:\n",
            "Process ForkPoolWorker-37:\n",
            "Process ForkPoolWorker-38:\n",
            "Process ForkPoolWorker-32:\n",
            "Process ForkPoolWorker-39:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "Process ForkPoolWorker-30:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
            "    res = self._reader.recv_bytes()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 401, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 401, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 401, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"<ipython-input-17-f96a0e41f473>\", line 30, in __getitem__\n",
            "    return self.__data_generation(list_IDs_temp)\n",
            "  File \"<ipython-input-17-f96a0e41f473>\", line 30, in __getitem__\n",
            "    return self.__data_generation(list_IDs_temp)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 401, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"<ipython-input-17-f96a0e41f473>\", line 30, in __getitem__\n",
            "    return self.__data_generation(list_IDs_temp)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"<ipython-input-17-f96a0e41f473>\", line 48, in __data_generation\n",
            "    Y[i] = (rgb2lab(self.datagen.random_transform(imread(os.path.join(os.path.join('data', self.partition + '-target'), ID)), seed=seed))\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "  File \"<ipython-input-17-f96a0e41f473>\", line 48, in __data_generation\n",
            "    Y[i] = (rgb2lab(self.datagen.random_transform(imread(os.path.join(os.path.join('data', self.partition + '-target'), ID)), seed=seed))\n",
            "  File \"<ipython-input-17-f96a0e41f473>\", line 30, in __getitem__\n",
            "    return self.__data_generation(list_IDs_temp)\n",
            "  File \"<ipython-input-17-f96a0e41f473>\", line 48, in __data_generation\n",
            "    Y[i] = (rgb2lab(self.datagen.random_transform(imread(os.path.join(os.path.join('data', self.partition + '-target'), ID)), seed=seed))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/preprocessing/image.py\", line 911, in random_transform\n",
            "    fill_mode=self.fill_mode, cval=self.cval)\n",
            "Traceback (most recent call last):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/skimage/color/colorconv.py\", line 1035, in rgb2lab\n",
            "    return xyz2lab(rgb2xyz(rgb), illuminant, observer)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 401, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "KeyboardInterrupt\n",
            "  File \"<ipython-input-17-f96a0e41f473>\", line 48, in __data_generation\n",
            "    Y[i] = (rgb2lab(self.datagen.random_transform(imread(os.path.join(os.path.join('data', self.partition + '-target'), ID)), seed=seed))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/preprocessing/image.py\", line 264, in apply_transform\n",
            "    cval=cval) for x_channel in x]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/preprocessing/image.py\", line 911, in random_transform\n",
            "    fill_mode=self.fill_mode, cval=self.cval)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/skimage/color/colorconv.py\", line 680, in rgb2xyz\n",
            "    arr[mask] = np.power((arr[mask] + 0.055) / 1.055, 2.4)\n",
            "  File \"<ipython-input-17-f96a0e41f473>\", line 30, in __getitem__\n",
            "    return self.__data_generation(list_IDs_temp)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/skimage/color/colorconv.py\", line 1035, in rgb2lab\n",
            "    return xyz2lab(rgb2xyz(rgb), illuminant, observer)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 401, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/preprocessing/image.py\", line 264, in apply_transform\n",
            "    cval=cval) for x_channel in x]\n",
            "KeyboardInterrupt\n",
            "  File \"<ipython-input-17-f96a0e41f473>\", line 47, in __data_generation\n",
            "    imread(os.path.join(os.path.join('data', self.partition), ID)), axis=-1), seed=seed) - 127.5) / 127.5\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/preprocessing/image.py\", line 264, in <listcomp>\n",
            "    cval=cval) for x_channel in x]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/preprocessing/image.py\", line 264, in <listcomp>\n",
            "    cval=cval) for x_channel in x]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/skimage/color/colorconv.py\", line 932, in xyz2lab\n",
            "    return np.concatenate([x[..., np.newaxis] for x in [L, a, b]], axis=-1)\n",
            "  File \"<ipython-input-17-f96a0e41f473>\", line 30, in __getitem__\n",
            "    return self.__data_generation(list_IDs_temp)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/numpy/lib/shape_base.py\", line 316, in expand_dims\n",
            "    if axis > a.ndim or axis < -a.ndim - 1:\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py\", line 470, in affine_transform\n",
            "    output, order, mode, cval, None, None)\n",
            "KeyboardInterrupt\n",
            "  File \"<ipython-input-17-f96a0e41f473>\", line 48, in __data_generation\n",
            "    Y[i] = (rgb2lab(self.datagen.random_transform(imread(os.path.join(os.path.join('data', self.partition + '-target'), ID)), seed=seed))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py\", line 470, in affine_transform\n",
            "    output, order, mode, cval, None, None)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py\", line 132, in _geometric_transform\n",
            "    order, mode, cval, extra_arguments, extra_keywords)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/preprocessing/image.py\", line 911, in random_transform\n",
            "    fill_mode=self.fill_mode, cval=self.cval)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py\", line 132, in _geometric_transform\n",
            "    order, mode, cval, extra_arguments, extra_keywords)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 401, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/preprocessing/image.py\", line 264, in apply_transform\n",
            "    cval=cval) for x_channel in x]\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/preprocessing/image.py\", line 264, in <listcomp>\n",
            "    cval=cval) for x_channel in x]\n",
            "  File \"<ipython-input-17-f96a0e41f473>\", line 30, in __getitem__\n",
            "    return self.__data_generation(list_IDs_temp)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py\", line 470, in affine_transform\n",
            "    output, order, mode, cval, None, None)\n",
            "  File \"<ipython-input-17-f96a0e41f473>\", line 48, in __data_generation\n",
            "    Y[i] = (rgb2lab(self.datagen.random_transform(imread(os.path.join(os.path.join('data', self.partition + '-target'), ID)), seed=seed))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py\", line 132, in _geometric_transform\n",
            "    order, mode, cval, extra_arguments, extra_keywords)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/skimage/color/colorconv.py\", line 1035, in rgb2lab\n",
            "    return xyz2lab(rgb2xyz(rgb), illuminant, observer)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/skimage/color/colorconv.py\", line 922, in xyz2lab\n",
            "    arr[mask] = np.power(arr[mask], 1. / 3.)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-bca6f32c367c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m         'color_model': 'mean_squared_error', 'clf_model': 'categorical_crossentropy'}, metrics={'color_model': 'accuracy', 'clf_model': 'accuracy'})\n\u001b[1;32m     31\u001b[0m     fmodel.fit_generator(generator=training_generator, epochs=20, verbose=1, callbacks=[\n\u001b[0;32m---> 32\u001b[0;31m         cbk], validation_data=validation_generator, use_multiprocessing=True, workers=4, initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training done.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2228\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2229\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2230\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2232\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "1zDuVeDohv3i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download weights and epochs"
      ]
    },
    {
      "metadata": {
        "id": "_gbMeea9hvQP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "e8197b0b-c22b-428c-e0cc-c03051519063"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('epochs.json')\n",
        "files.download('weights.h5')\n",
        "files.download('mapping.json')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "Exception happened during processing of request from ('::ffff:127.0.0.1', 45002, 0, 0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 317, in _handle_request_noblock\n",
            "    self.process_request(request, client_address)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 348, in process_request\n",
            "    self.finish_request(request, client_address)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 361, in finish_request\n",
            "    self.RequestHandlerClass(request, client_address, self)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 696, in __init__\n",
            "    self.handle()\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 418, in handle\n",
            "    self.handle_one_request()\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 406, in handle_one_request\n",
            "    method()\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 639, in do_GET\n",
            "    self.copyfile(f, self.wfile)\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 800, in copyfile\n",
            "    shutil.copyfileobj(source, outputfile)\n",
            "  File \"/usr/lib/python3.6/shutil.py\", line 82, in copyfileobj\n",
            "    fdst.write(buf)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 775, in write\n",
            "    self._sock.sendall(b)\n",
            "ConnectionResetError: [Errno 104] Connection reset by peer\n",
            "----------------------------------------\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}